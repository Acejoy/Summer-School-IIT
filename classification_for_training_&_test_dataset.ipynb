{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import mahotas as mt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skimage.feature import greycomatrix,greycoprops,hog\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glcm features\n",
    "\n",
    "def extract_glcm_features(image_patch):\n",
    "    \n",
    "    #feature extraction using GLCM\n",
    "    \n",
    "    grey_patch=cv2.cvtColor(image_patch,cv2.COLOR_BGR2GRAY)\n",
    "    glcm_mat=greycomatrix(grey_patch,[1],[0,np.pi/4,np.pi,0.75*np.pi],levels=256,symmetric=True,normed=True)\n",
    "    #different features\n",
    "    contrast=np.mean(greycoprops(glcm_mat,'contrast'))\n",
    "        \n",
    "    dissimilar=np.mean(greycoprops(glcm_mat,'dissimilarity'))\n",
    "        \n",
    "    homogeneity=np.mean(greycoprops(glcm_mat,'homogeneity'))\n",
    "        \n",
    "    energy=np.mean(greycoprops(glcm_mat,'energy'))\n",
    "        \n",
    "    asm=np.mean(greycoprops(glcm_mat,'ASM'))\n",
    "        \n",
    "    correlation=np.mean(greycoprops(glcm_mat,'correlation'))\n",
    "        \n",
    "    feature_vect=[correlation,dissimilar,homogeneity,energy,asm,correlation]\n",
    "    \n",
    "    \n",
    "    return feature_vect\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the train dataset along with labels\n",
    "\n",
    "gt_folder_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Training01\\Ground Truth'\n",
    "\n",
    "orig_folder_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Training01\\Original Image'\n",
    "\n",
    "segments = ['Bicycle', 'Car', 'Human']\n",
    "\n",
    "\n",
    "#lists of data\n",
    "train_label = []\n",
    "glcm_train_data = []\n",
    "hog_train_data = []\n",
    "brisk_train_data = []\n",
    "\n",
    "humoments_train_data = []\n",
    "haralick_train_data = []\n",
    "#encoding the label names into numbers\n",
    "#human is 0\n",
    "#car is 1\n",
    "#bicycle is 2\n",
    "\n",
    "encoded ={'human':0, 'car':1, 'bicycle':2}\n",
    "\n",
    "\n",
    "#describing the HOG descriptor\n",
    "hog = cv2.HOGDescriptor()\n",
    "\n",
    "#initialising the brisk descriptor\n",
    "brisk = cv2.BRISK_create()\n",
    "\n",
    "\n",
    "for segment in segments:\n",
    "    \n",
    "    image_folder = os.path.join(orig_folder_path,segment)\n",
    "    gt_image_folder = os.path.join(gt_folder_path,segment)\n",
    "    \n",
    "    \n",
    "    list_orig = os.listdir(image_folder)\n",
    "    list_gt = os.listdir(gt_image_folder)\n",
    "    \n",
    "    for orig_name,gt_name in zip(list_orig,list_gt):\n",
    "        \n",
    "        orig_image = cv2.imread(os.path.join(image_folder,orig_name),1)\n",
    "        gt_image = cv2.imread(os.path.join(gt_image_folder,gt_name),0)\n",
    "        \n",
    "        if segment == 'Bicycle':\n",
    "            gt_image = np.invert(gt_image)\n",
    "            train_label.append(2)\n",
    "        \n",
    "        if segment == 'Human':\n",
    "            train_label.append(0)\n",
    "            \n",
    "        if segment == 'Car':\n",
    "            train_label.append(1)\n",
    "            \n",
    "        #edged = imutils.auto_canny(gt_image)    \n",
    "        contours,hierarchies = cv2.findContours(gt_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        max_cnt=max(contours,key=cv2.contourArea)\n",
    "            \n",
    "        max_rect=cv2.boundingRect(max_cnt)\n",
    "        x,y,w,h = max_rect\n",
    "        \n",
    "        '''\n",
    "        cv2.rectangle(orig_image,(x,y),(x+w,y+h),(0,0,255),1)\n",
    "        \n",
    "        cv2.imshow(orig_name,orig_image)\n",
    "        k=cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if k == ord('q'):\n",
    "            cv2.destroyWindow(orig_name)\n",
    "            break\n",
    "        else:\n",
    "            cv2.destroyWindow(orig_name)\n",
    "         \n",
    "        #cv2.destroyWindow(orig_name)\n",
    "        '''\n",
    "        \n",
    "        image_patch=orig_image[y:y+h, x:x+w ]\n",
    "        #resizing to 64 X 128\n",
    "        #print(orig_name,x,y,w,h)\n",
    "        \n",
    "        resized_patch = cv2.resize(image_patch,(64,128))\n",
    "        \n",
    "        edged_patch = cv2.Canny(resized_patch,200,230)\n",
    "        \n",
    "        '''\n",
    "        cv2.imshow('edge',edged_patch)\n",
    "        cv2.imshow('orig',image_patch)\n",
    "        k=cv2.waitKey(0) & 0xFF\n",
    "        \n",
    "        if k == ord('q'):\n",
    "            cv2.destroyWindow(orig_name)\n",
    "            cv2.destroyWindow('edge')\n",
    "            cv2.destroyWindow('orig')\n",
    "            break\n",
    "        else:\n",
    "            cv2.destroyWindow(orig_name)\n",
    "            cv2.destroyWindow('edge')\n",
    "            cv2.destroyWindow('orig')\n",
    "        '''\n",
    "        \n",
    "        #extracting features\n",
    "        #extracting the glcm features\n",
    "        \n",
    "        glcm_train_data.append(np.asarray(extract_glcm_features(resized_patch)))\n",
    "        \n",
    "        #extract hog features\n",
    "        hog_fetures = hog.compute(cv2.cvtColor(resized_patch,cv2.COLOR_BGR2GRAY))\n",
    "        hog_train_data.append(hog_fetures.reshape(3780))\n",
    "        \n",
    "        humoments_train_data.append(cv2.HuMoments(cv2.moments(gt_image[y:y+h,x:x+w])))\n",
    "        \n",
    "        haralick_train_data.append(mt.features.haralick(resized_patch))\n",
    "        '''\n",
    "        #extract brisk features\n",
    "        keypoint , features = brisk.detectAndCompute(resized_patch,None)\n",
    "        brisk_train_data.append(features.flatten())\n",
    "        '''\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.06788008e-03]\n",
      " [ 3.11866066e-06]\n",
      " [ 3.46375209e-09]\n",
      " [ 2.04147016e-09]\n",
      " [ 5.42201217e-18]\n",
      " [ 3.60278069e-12]\n",
      " [-2.67389931e-19]]\n",
      "(248, 7)\n"
     ]
    }
   ],
   "source": [
    "humoments_train_data\n",
    "k= np.asarray(humoments_train_data)\n",
    "print(humoments_train_data[0])\n",
    "k=k.reshape(-1,7)\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomise the datasets: labels and data\n",
    "randomise = np.arange(len(hog_train_data))\n",
    "np.random.shuffle(randomise)\n",
    "\n",
    "hog_train_data = np.asarray(hog_train_data)\n",
    "hog_train_data = hog_train_data[randomise]\n",
    "\n",
    "glcm_train_data = np.asarray(glcm_train_data)\n",
    "glcm_train_data = glcm_train_data[randomise]\n",
    "\n",
    "humoments_train_data = np.asarray(humoments_train_data)\n",
    "humoments_train_data = humoments_train_data.reshape(-1,7)\n",
    "humoments_train_data = humoments_train_data[randomise]\n",
    "\n",
    "\n",
    "haralick_train_data = np.asarray(haralick_train_data)\n",
    "haralick_train_data = haralick_train_data.reshape(248,-1)\n",
    "haralick_train_data = haralick_train_data[randomise]\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "train_label = train_label[randomise]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 169)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haralick_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n",
      "(1, 169)\n"
     ]
    }
   ],
   "source": [
    "linsvm=SVC(kernel='linear',gamma='auto')\n",
    "polysvm = SVC(kernel = 'poly',gamma='auto')\n",
    "rbfsvm = SVC(kernel='rbf',gamma='auto')\n",
    "\n",
    "'''\n",
    "linsvm.fit(haralick_train_data,train_label)\n",
    "polysvm.fit(haralick_train_data, train_label)\n",
    "rbfsvm.fit(haralick_train_data,train_label)\n",
    "print()\n",
    "\n",
    "\n",
    "linsvm.fit(humoments_train_data,train_label)\n",
    "polysvm.fit(humoments_train_data, train_label)\n",
    "rbfsvm.fit(humoments_train_data,train_label)\n",
    "print()\n",
    "\n",
    "linsvm.fit(hog_train_data,train_label)\n",
    "polysvm.fit(hog_train_data, train_label)\n",
    "rbfsvm.fit(hog_train_data,train_label)\n",
    "\n",
    "'''\n",
    "linsvm.fit(glcm_train_data,train_label)\n",
    "polysvm.fit(glcm_train_data, train_label)\n",
    "rbfsvm.fit(glcm_train_data,train_label)\n",
    "\n",
    "\n",
    "orig_test_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Testing01\\Original'\n",
    "gt_test_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Testing01\\GT'\n",
    "\n",
    "#gt_test_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Training01\\Ground Truth\\Car'\n",
    "#orig_test_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Training01\\Original Image\\Car'\n",
    "\n",
    "list_orig_test = os.listdir(orig_test_path)\n",
    "list_gt_test = os.listdir(gt_test_path)\n",
    "\n",
    "#print(list_orig_test)\n",
    "#print(list_gt_test)\n",
    "bbox_encoding = {0:(0,0,255), 1:(0,255,0), 2:(255,0,0)}\n",
    "\n",
    "hog=cv2.HOGDescriptor()\n",
    "\n",
    "for orig_test_image_name,gt_test_image_name in zip(list_orig_test,list_gt_test):\n",
    "    \n",
    "    orig_test_image = cv2.imread(os.path.join(orig_test_path,orig_test_image_name),1)\n",
    "    #print(os.path.join(orig_test_path,orig_test_image_name))\n",
    "    \n",
    "    gt_test_image = cv2.imread(os.path.join(gt_test_path,gt_test_image_name),0)\n",
    "    #gt_test_image = np.invert(gt_test_image)\n",
    "    #print(os.path.join(gt_test_path,gt_test_image_name))\n",
    "    \n",
    "    \n",
    "    contours,hierarchies = cv2.findContours(gt_test_image , cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rects = [cv2.boundingRect(cnt) for cnt in contours]\n",
    "    \n",
    "    patches = []\n",
    "    \n",
    "    for rect in rects:\n",
    "        x,y,w,h = rect\n",
    "        patch = orig_test_image[y:y+h,x:x+w]\n",
    "        resized_patch = cv2.resize(patch,(64,128))\n",
    "        \n",
    "        glcm_feature = np.asarray(extract_glcm_features(resized_patch))\n",
    "        #print(glcm_feature.shape)\n",
    "        hog_features=hog.compute(resized_patch).reshape(3780)\n",
    "        \n",
    "        humoment_data = cv2.HuMoments(cv2.moments(gt_test_image[y:y+h,x:x+w]))\n",
    "        humoment_data = humoment_data.reshape(-1,7)\n",
    "        \n",
    "        haralick_feature = mt.features.haralick(resized_patch)\n",
    "        haralick_feature = haralick_feature.reshape(1,-1)\n",
    "        print(haralick_feature.shape)\n",
    "        \n",
    "        y_pred = rbfsvm.predict([glcm_feature])\n",
    "        \n",
    "        #print(y_pred[0])\n",
    "        \n",
    "        cv2.rectangle(orig_test_image, (x,y),(x+w,y+h),bbox_encoding[y_pred[0]],1)\n",
    "    cv2.imshow(orig_test_image_name,orig_test_image)\n",
    "    k = cv2.waitKey(0) & 0xFF\n",
    "    if k== ord('q'):\n",
    "        cv2.destroyWindow(orig_test_image_name)\n",
    "        break\n",
    "    else :\n",
    "        cv2.destroyWindow(orig_test_image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SVC in module sklearn.svm.classes:\n",
      "\n",
      "class SVC(sklearn.svm.base.BaseSVC)\n",
      " |  SVC(C=1.0, kernel='rbf', degree=3, gamma='auto_deprecated', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
      " |  \n",
      " |  C-Support Vector Classification.\n",
      " |  \n",
      " |  The implementation is based on libsvm. The fit time complexity\n",
      " |  is more than quadratic with the number of samples which makes it hard\n",
      " |  to scale to dataset with more than a couple of 10000 samples.\n",
      " |  \n",
      " |  The multiclass support is handled according to a one-vs-one scheme.\n",
      " |  \n",
      " |  For details on the precise mathematical formulation of the provided\n",
      " |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      " |  other, see the corresponding section in the narrative documentation:\n",
      " |  :ref:`svm_kernels`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  C : float, optional (default=1.0)\n",
      " |      Penalty parameter C of the error term.\n",
      " |  \n",
      " |  kernel : string, optional (default='rbf')\n",
      " |      Specifies the kernel type to be used in the algorithm.\n",
      " |      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      " |      a callable.\n",
      " |      If none is given, 'rbf' will be used. If a callable is given it is\n",
      " |      used to pre-compute the kernel matrix from data matrices; that matrix\n",
      " |      should be an array of shape ``(n_samples, n_samples)``.\n",
      " |  \n",
      " |  degree : int, optional (default=3)\n",
      " |      Degree of the polynomial kernel function ('poly').\n",
      " |      Ignored by all other kernels.\n",
      " |  \n",
      " |  gamma : float, optional (default='auto')\n",
      " |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |      Current default is 'auto' which uses 1 / n_features,\n",
      " |      if ``gamma='scale'`` is passed then it uses 1 / (n_features * X.std())\n",
      " |      as value of gamma. The current default of gamma, 'auto', will change\n",
      " |      to 'scale' in version 0.22. 'auto_deprecated', a deprecated version of\n",
      " |      'auto' is used as a default indicating that no explicit value of gamma\n",
      " |      was passed.\n",
      " |  \n",
      " |  coef0 : float, optional (default=0.0)\n",
      " |      Independent term in kernel function.\n",
      " |      It is only significant in 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |  shrinking : boolean, optional (default=True)\n",
      " |      Whether to use the shrinking heuristic.\n",
      " |  \n",
      " |  probability : boolean, optional (default=False)\n",
      " |      Whether to enable probability estimates. This must be enabled prior\n",
      " |      to calling `fit`, and will slow down that method.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-3)\n",
      " |      Tolerance for stopping criterion.\n",
      " |  \n",
      " |  cache_size : float, optional\n",
      " |      Specify the size of the kernel cache (in MB).\n",
      " |  \n",
      " |  class_weight : {dict, 'balanced'}, optional\n",
      " |      Set the parameter C of class i to class_weight[i]*C for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : bool, default: False\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  max_iter : int, optional (default=-1)\n",
      " |      Hard limit on iterations within solver, or -1 for no limit.\n",
      " |  \n",
      " |  decision_function_shape : 'ovo', 'ovr', default='ovr'\n",
      " |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      " |      (n_samples, n_classes) as all other classifiers, or the original\n",
      " |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      " |      (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n",
      " |      ('ovo') is always used as multi-class strategy.\n",
      " |  \n",
      " |      .. versionchanged:: 0.19\n",
      " |          decision_function_shape is 'ovr' by default.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *decision_function_shape='ovr'* is recommended.\n",
      " |  \n",
      " |      .. versionchanged:: 0.17\n",
      " |         Deprecated *decision_function_shape='ovo' and None*.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      The seed of the pseudo random number generator used when shuffling\n",
      " |      the data for probability estimates. If int, random_state is the\n",
      " |      seed used by the random number generator; If RandomState instance,\n",
      " |      random_state is the random number generator; If None, the random\n",
      " |      number generator is the RandomState instance used by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  support_ : array-like, shape = [n_SV]\n",
      " |      Indices of support vectors.\n",
      " |  \n",
      " |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
      " |      Support vectors.\n",
      " |  \n",
      " |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
      " |      Number of support vectors for each class.\n",
      " |  \n",
      " |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
      " |      Coefficients of the support vector in the decision function.\n",
      " |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      " |      The layout of the coefficients in the multiclass case is somewhat\n",
      " |      non-trivial. See the section about multi-class classification in the\n",
      " |      SVM section of the User Guide for details.\n",
      " |  \n",
      " |  coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      " |      `support_vectors_`.\n",
      " |  \n",
      " |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  fit_status_ : int\n",
      " |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      " |  \n",
      " |  probA_ : array, shape = [n_class * (n_class-1) / 2]\n",
      " |  probB_ : array, shape = [n_class * (n_class-1) / 2]\n",
      " |      If probability=True, the parameters learned in Platt scaling to\n",
      " |      produce probability estimates from decision values. If\n",
      " |      probability=False, an empty array. Platt scaling uses the logistic\n",
      " |      function\n",
      " |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
      " |      where ``probA_`` and ``probB_`` are learned from the dataset. For more\n",
      " |      information on the multiclass case and training procedure see section\n",
      " |      8 of LIBSVM: A Library for Support Vector Machines (in References)\n",
      " |      for more.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      " |  >>> y = np.array([1, 1, 2, 2])\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> clf = SVC(gamma='auto')\n",
      " |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      " |  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      " |      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      " |      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      " |      tol=0.001, verbose=False)\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SVR\n",
      " |      Support Vector Machine for Regression implemented using libsvm.\n",
      " |  \n",
      " |  LinearSVC\n",
      " |      Scalable Linear Support Vector Machine for classification\n",
      " |      implemented using liblinear. Check the See also section of\n",
      " |      LinearSVC for more comparison element.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  **References:**\n",
      " |  `LIBSVM: A Library for Support Vector Machines\n",
      " |  <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SVC\n",
      " |      sklearn.svm.base.BaseSVC\n",
      " |      abc.NewBase\n",
      " |      sklearn.svm.base.BaseLibSVM\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto_deprecated', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Distance of the samples X to the separating hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          Returns the decision function of the sample for each class\n",
      " |          in the model.\n",
      " |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      " |          n_classes)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on samples in X.\n",
      " |      \n",
      " |      For an one-class model, +1 or -1 is returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array, shape (n_samples,)\n",
      " |          Class labels for samples in X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
      " |  \n",
      " |  predict_log_proba\n",
      " |      Compute log probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the log-probabilities of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Compute probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the SVM model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples, n_samples).\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values (class labels in classification, real numbers in\n",
      " |          regression)\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,)\n",
      " |          Per-sample weights. Rescale C per sample. Higher weights\n",
      " |          force the classifier to put more emphasis on these points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |      \n",
      " |      Notes\n",
      " |      ------\n",
      " |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      " |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      " |      \n",
      " |      If X is a dense array, then the other methods will not support sparse\n",
      " |      matrices as input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the test dataset along with labels\n",
    "\n",
    "gt_folder_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Training01\\Ground Truth'\n",
    "\n",
    "orig_folder_path = r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Training01\\Original Image'\n",
    "\n",
    "segments = ['Bicycle', 'Car', 'Human']\n",
    "\n",
    "\n",
    "#lists of data\n",
    "train_label = []\n",
    "glcm_train_data = []\n",
    "hog_train_data = []\n",
    "brisk_train_data = []\n",
    "humoments_train_data = []\n",
    "\n",
    "#encoding the label names into numbers\n",
    "#human is 1 and bb color is blue\n",
    "#car is 0 and bb color is red\n",
    "#bicycle is 2 and bb color is green\n",
    "\n",
    "encoded ={'human':0, 'car':1, 'bicycle':2}\n",
    "\n",
    "\n",
    "#describing the HOG descriptor\n",
    "hog = cv2.HOGDescriptor()\n",
    "\n",
    "#initialising the brisk descriptor\n",
    "brisk = cv2.BRISK_create()\n",
    "\n",
    "\n",
    "for orig_name,gt_name in zip(list_orig,list_gt):\n",
    "    \n",
    "    image_folder = os.path.join(orig_folder_path,segment)\n",
    "    gt_image_folder = os.path.join(gt_folder_path,segment)\n",
    "    \n",
    "    \n",
    "    list_orig = os.listdir(image_folder)\n",
    "    list_gt = os.listdir(gt_image_folder)\n",
    "    \n",
    "    for orig_name,gt_name in zip(list_orig,list_gt):\n",
    "        \n",
    "        orig_image = cv2.imread(os.path.join(image_folder,orig_name),1)\n",
    "        gt_image = cv2.imread(os.path.join(gt_image_folder,gt_name),0)\n",
    "        \n",
    "        if segment == 'Bicycle':\n",
    "            gt_image = np.invert(gt_image)\n",
    "            train_label.append(2)\n",
    "        \n",
    "        if segment == 'Human':\n",
    "            train_label.append(0)\n",
    "            \n",
    "        if segment == 'Car':\n",
    "            train_label.append(1)\n",
    "            \n",
    "            \n",
    "        contours,hierarchies = cv2.findContours(gt_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        max_cnt=max(contours,key=cv2.contourArea)\n",
    "            \n",
    "        max_rect=cv2.boundingRect(max_cnt)\n",
    "        x,y,w,h = max_rect\n",
    "        \n",
    "        \n",
    "        cv2.rectangle(orig_image,(x,y),(x+w,y+h),(0,0,255),1)\n",
    "        cv2.imshow(orig_name,orig_image)\n",
    "        k=cv2.waitKey(1) & 0xFF\n",
    "        '''\n",
    "        if k == ord('q'):\n",
    "            cv2.destroyWindow(orig_name)\n",
    "            break\n",
    "        else:\n",
    "            cv2.destroyWindow(orig_name)\n",
    "        ''' \n",
    "        cv2.destroyWindow(orig_name)\n",
    "        \n",
    "        image_patch=orig_image[y:y+h, x:x+w ]\n",
    "        #resizing to 64 X 128\n",
    "        print(orig_name,x,y,w,h)\n",
    "        resized_patch = cv2.resize(image_patch,(64,128))\n",
    "        \n",
    "        #extracting features\n",
    "        #extracting the glcm features\n",
    "        glcm_train_data.append(np.asarray(extract_glcm_features(resized_patch)))\n",
    "        \n",
    "        #extract hog features\n",
    "        hog_fetures = hog.compute(resized_patch)\n",
    "        hog_train_data.append(hog_fetures.reshape(3780))\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        #extract brisk features\n",
    "        keypoint , features = brisk.detectAndCompute(resized_patch,None)\n",
    "        brisk_train_data.append(features.flatten())\n",
    "        '''\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the classifiers\n",
    "\n",
    "#linear svm\n",
    "\n",
    "def lin_svm(train_data, train_label, test_data, test_label):\n",
    "    \n",
    "    linearsvmclassifier = SVC(kernel='linear')\n",
    "    linearsvmclassifier.fit(train_data, train_label)\n",
    "    \n",
    "    predict_labels = linearsvmclassifier.predict(test_data)\n",
    "    \n",
    "    print('the confusion matrix is:\\n',confusion_matrix(test_label, predict_labels))\n",
    "    print('the classification report is as follows:\\n',classification_report(test_label, predict_labels))\n",
    "    print('the accuracy is:' , metrics.accuracy_score(test_label, predict_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "#rbf svm\n",
    "def rbf_svm(train_data, train_label, test_data, test_label):\n",
    "    \n",
    "    rbfsvmclassifier = SVC(kernel='rbf')\n",
    "    rbfsvmclassifier.fit(train_data, train_label)\n",
    "    \n",
    "    predict_labels = rbfsvmclassifier.predict(test_data)\n",
    "    \n",
    "    print('the confusion matrix is:\\n',confusion_matrix(test_label, predict_labels))\n",
    "    print('the classification report is as follows:\\n',classification_report(test_label, predict_labels))\n",
    "    print('the accuracy is:' , metrics.accuracy_score(test_label, predict_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[0.4045094  0.04776263 0.02173951 ... 0.08400174 0.05908861 0.05564167]\n",
      "(3780,)\n"
     ]
    }
   ],
   "source": [
    "resized_patch.shape\n",
    "hog=cv2.HOGDescriptor()\n",
    "k=hog.compute(resized_patch)\n",
    "k=k.reshape(3780)\n",
    "print(type(k))\n",
    "print(k)\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 160, 3)\n",
      "(48, 26, 3)\n",
      "(7, 64)\n",
      "(448,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path=r'C:\\Users\\Legion\\Desktop\\Internship_IIT\\Training & esting Dataset\\Datasets\\Training01\\Original Image\\Human\\b02880.bmp'\n",
    "\n",
    "\n",
    "bimage=cv2.imread(path,1)\n",
    "im_patch=bimage[13:61,107:133]\n",
    "\n",
    "resized= cv2.resize(bimage,(64,128))\n",
    "print(bimage.shape)\n",
    "print(im_patch.shape)\n",
    "\n",
    "cv2.imshow('bike',bimage)\n",
    "\n",
    "cv2.imshow('patch',im_patch)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "brisk = cv2.BRISK_create()\n",
    "k,f = brisk.detectAndCompute(resized,None)\n",
    "print(f.shape)\n",
    "f=f.flatten()\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HOGDescriptor in module cv2.cv2:\n",
      "\n",
      "class HOGDescriptor(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  checkDetectorSize(...)\n",
      " |      checkDetectorSize() -> retval\n",
      " |      .   @brief Checks if detector size equal to descriptor size.\n",
      " |  \n",
      " |  compute(...)\n",
      " |      compute(img[, winStride[, padding[, locations]]]) -> descriptors\n",
      " |      .   @brief Computes HOG descriptors of given image.\n",
      " |      .   @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.\n",
      " |      .   @param descriptors Matrix of the type CV_32F\n",
      " |      .   @param winStride Window stride. It must be a multiple of block stride.\n",
      " |      .   @param padding Padding\n",
      " |      .   @param locations Vector of Point\n",
      " |  \n",
      " |  computeGradient(...)\n",
      " |      computeGradient(img, grad, angleOfs[, paddingTL[, paddingBR]]) -> grad, angleOfs\n",
      " |      .   @brief  Computes gradients and quantized gradient orientations.\n",
      " |      .   @param img Matrix contains the image to be computed\n",
      " |      .   @param grad Matrix of type CV_32FC2 contains computed gradients\n",
      " |      .   @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations\n",
      " |      .   @param paddingTL Padding from top-left\n",
      " |      .   @param paddingBR Padding from bottom-right\n",
      " |  \n",
      " |  detect(...)\n",
      " |      detect(img[, hitThreshold[, winStride[, padding[, searchLocations]]]]) -> foundLocations, weights\n",
      " |      .   @brief Performs object detection without a multi-scale window.\n",
      " |      .   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n",
      " |      .   @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.\n",
      " |      .   @param weights Vector that will contain confidence values for each detected object.\n",
      " |      .   @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n",
      " |      .   Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n",
      " |      .   But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n",
      " |      .   @param winStride Window stride. It must be a multiple of block stride.\n",
      " |      .   @param padding Padding\n",
      " |      .   @param searchLocations Vector of Point includes set of requested locations to be evaluated.\n",
      " |  \n",
      " |  detectMultiScale(...)\n",
      " |      detectMultiScale(img[, hitThreshold[, winStride[, padding[, scale[, finalThreshold[, useMeanshiftGrouping]]]]]]) -> foundLocations, foundWeights\n",
      " |      .   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n",
      " |      .   of rectangles.\n",
      " |      .   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n",
      " |      .   @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n",
      " |      .   @param foundWeights Vector that will contain confidence values for each detected object.\n",
      " |      .   @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n",
      " |      .   Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n",
      " |      .   But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n",
      " |      .   @param winStride Window stride. It must be a multiple of block stride.\n",
      " |      .   @param padding Padding\n",
      " |      .   @param scale Coefficient of the detection window increase.\n",
      " |      .   @param finalThreshold Final threshold\n",
      " |      .   @param useMeanshiftGrouping indicates grouping algorithm\n",
      " |  \n",
      " |  getDescriptorSize(...)\n",
      " |      getDescriptorSize() -> retval\n",
      " |      .   @brief Returns the number of coefficients required for the classification.\n",
      " |  \n",
      " |  getWinSigma(...)\n",
      " |      getWinSigma() -> retval\n",
      " |      .   @brief Returns winSigma value\n",
      " |  \n",
      " |  load(...)\n",
      " |      load(filename[, objname]) -> retval\n",
      " |      .   @brief loads HOGDescriptor parameters and coefficients for the linear SVM classifier from a file.\n",
      " |      .   @param filename Path of the file to read.\n",
      " |      .   @param objname The optional name of the node to read (if empty, the first top-level node will be used).\n",
      " |  \n",
      " |  save(...)\n",
      " |      save(filename[, objname]) -> None\n",
      " |      .   @brief saves HOGDescriptor parameters and coefficients for the linear SVM classifier to a file\n",
      " |      .   @param filename File name\n",
      " |      .   @param objname Object name\n",
      " |  \n",
      " |  setSVMDetector(...)\n",
      " |      setSVMDetector(svmdetector) -> None\n",
      " |      .   @brief Sets coefficients for the linear SVM classifier.\n",
      " |      .   @param svmdetector coefficients for the linear SVM classifier.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  getDaimlerPeopleDetector(...)\n",
      " |      getDaimlerPeopleDetector() -> retval\n",
      " |      .   @brief Returns coefficients of the classifier trained for people detection (for 48x96 windows).\n",
      " |  \n",
      " |  getDefaultPeopleDetector(...)\n",
      " |      getDefaultPeopleDetector() -> retval\n",
      " |      .   @brief Returns coefficients of the classifier trained for people detection (for 64x128 windows).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  L2HysThreshold\n",
      " |      L2HysThreshold\n",
      " |  \n",
      " |  blockSize\n",
      " |      blockSize\n",
      " |  \n",
      " |  blockStride\n",
      " |      blockStride\n",
      " |  \n",
      " |  cellSize\n",
      " |      cellSize\n",
      " |  \n",
      " |  derivAperture\n",
      " |      derivAperture\n",
      " |  \n",
      " |  gammaCorrection\n",
      " |      gammaCorrection\n",
      " |  \n",
      " |  histogramNormType\n",
      " |      histogramNormType\n",
      " |  \n",
      " |  nbins\n",
      " |      nbins\n",
      " |  \n",
      " |  nlevels\n",
      " |      nlevels\n",
      " |  \n",
      " |  signedGradient\n",
      " |      signedGradient\n",
      " |  \n",
      " |  svmDetector\n",
      " |      svmDetector\n",
      " |  \n",
      " |  winSigma\n",
      " |      winSigma\n",
      " |  \n",
      " |  winSize\n",
      " |      winSize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.HOGDescriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
